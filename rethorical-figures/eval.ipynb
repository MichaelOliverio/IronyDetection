{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bed35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in generations/\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e17e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in generations directory\n",
    "def open_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35763be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline decoding-1\n",
      "LLaMAntino-3-ANITA-8B-Inst-DPO-ITA decoding-1\n",
      "LLaMAntino-3-ANITA-8B-Inst-DPO-ITA decoding-2\n",
      "LLaMAntino-3-ANITA-8B-Inst-DPO-ITA decoding-3\n",
      "Llama-3.1-8B-Instruct decoding-1\n",
      "Llama-3.1-8B-Instruct decoding-2\n",
      "Llama-3.1-8B-Instruct decoding-3\n",
      "Minerva-7B-instruct-v1.0 decoding-1\n",
      "Minerva-7B-instruct-v1.0 decoding-2\n",
      "Minerva-7B-instruct-v1.0 decoding-3\n",
      "Ministral-8B-Instruct-2410 decoding-1\n",
      "Ministral-8B-Instruct-2410 decoding-2\n",
      "Qwen2.5-7B-Instruct decoding-1\n",
      "Qwen2.5-7B-Instruct decoding-2\n",
      "Qwen2.5-7B-Instruct decoding-3\n"
     ]
    }
   ],
   "source": [
    "files = open_files('generations')\n",
    "\n",
    "models_generations = {}\n",
    "for file in files:\n",
    "    model = model = re.sub(r\"^fine-tuned-|-decoding-\\d+.csv\", \"\", file)\n",
    "    gen = re.search(r\"decoding-\\d+\", file).group()\n",
    "    print(model, gen)\n",
    "    if model == 'baseline':\n",
    "        continue\n",
    "\n",
    "    if model not in models_generations:\n",
    "        models_generations[model] = {}\n",
    "    if gen not in models_generations[model]:\n",
    "        models_generations[model][gen] = {}\n",
    "\n",
    "    models_generations[model][gen] = pd.read_csv('generations/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16c8cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fccd4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "RHETORICAL QUESTION     0.6429    0.8182    0.7200        22\n",
      "          HYPERBOLE     0.2727    0.3750    0.3158         8\n",
      "              OTHER     0.2917    0.3500    0.3182        20\n",
      "            ANALOGY     0.5600    0.5385    0.5490        26\n",
      "          EUPHEMISM     0.0000    0.0000    0.0000         8\n",
      " IM:FALSE ASSERTION     0.4000    0.1667    0.2353        12\n",
      "   EX:CONTEXT SHIFT     0.1538    0.1111    0.1290        18\n",
      "EX:OXYMORON PARADOX     0.2941    0.3571    0.3226        28\n",
      "\n",
      "          micro avg     0.4000    0.3944    0.3972       142\n",
      "          macro avg     0.3269    0.3396    0.3237       142\n",
      "       weighted avg     0.3699    0.3944    0.3745       142\n",
      "\n",
      "Llama-3.1-8B-Instruct\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "RHETORICAL QUESTION     0.6522    0.6818    0.6667        22\n",
      "          HYPERBOLE     0.4286    0.3750    0.4000         8\n",
      "              OTHER     0.2308    0.3000    0.2609        20\n",
      "            ANALOGY     0.5652    0.5000    0.5306        26\n",
      "          EUPHEMISM     0.0000    0.0000    0.0000         8\n",
      " IM:FALSE ASSERTION     0.2500    0.0833    0.1250        12\n",
      "   EX:CONTEXT SHIFT     0.2222    0.2222    0.2222        18\n",
      "EX:OXYMORON PARADOX     0.3235    0.3929    0.3548        28\n",
      "\n",
      "           accuracy                         0.3732       142\n",
      "          macro avg     0.3341    0.3194    0.3200       142\n",
      "       weighted avg     0.3743    0.3732    0.3684       142\n",
      "\n",
      "Minerva-7B-instruct-v1.0\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "RHETORICAL QUESTION     0.6667    0.8182    0.7347        22\n",
      "          HYPERBOLE     0.2000    0.1250    0.1538         8\n",
      "              OTHER     0.2083    0.2500    0.2273        20\n",
      "            ANALOGY     0.6667    0.6154    0.6400        26\n",
      "          EUPHEMISM     0.0000    0.0000    0.0000         8\n",
      " IM:FALSE ASSERTION     0.2727    0.2500    0.2609        12\n",
      "   EX:CONTEXT SHIFT     0.2500    0.2222    0.2353        18\n",
      "EX:OXYMORON PARADOX     0.2500    0.2857    0.2667        28\n",
      "\n",
      "           accuracy                         0.3873       142\n",
      "          macro avg     0.3143    0.3208    0.3148       142\n",
      "       weighted avg     0.3700    0.3873    0.3761       142\n",
      "\n",
      "Ministral-8B-Instruct-2410\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "RHETORICAL QUESTION     0.6000    0.8182    0.6923        22\n",
      "          HYPERBOLE     0.5000    0.3750    0.4286         8\n",
      "              OTHER     0.2941    0.2500    0.2703        20\n",
      "            ANALOGY     0.5357    0.5769    0.5556        26\n",
      "          EUPHEMISM     0.0000    0.0000    0.0000         8\n",
      " IM:FALSE ASSERTION     0.3000    0.2500    0.2727        12\n",
      "   EX:CONTEXT SHIFT     0.2143    0.1667    0.1875        18\n",
      "EX:OXYMORON PARADOX     0.2903    0.3214    0.3051        28\n",
      "\n",
      "          micro avg     0.4000    0.3944    0.3972       142\n",
      "          macro avg     0.3418    0.3448    0.3390       142\n",
      "       weighted avg     0.3704    0.3944    0.3782       142\n",
      "\n",
      "Qwen2.5-7B-Instruct\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "RHETORICAL QUESTION     0.5556    0.6818    0.6122        22\n",
      "          HYPERBOLE     0.1429    0.1250    0.1333         8\n",
      "              OTHER     0.1765    0.1500    0.1622        20\n",
      "            ANALOGY     0.5769    0.5769    0.5769        26\n",
      "          EUPHEMISM     0.0000    0.0000    0.0000         8\n",
      " IM:FALSE ASSERTION     0.2857    0.1667    0.2105        12\n",
      "   EX:CONTEXT SHIFT     0.1905    0.2222    0.2051        18\n",
      "EX:OXYMORON PARADOX     0.3548    0.3929    0.3729        28\n",
      "\n",
      "           accuracy                         0.3592       142\n",
      "          macro avg     0.2854    0.2894    0.2841       142\n",
      "       weighted avg     0.3429    0.3592    0.3482       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [\n",
    "    'RHETORICAL QUESTION',\n",
    "    'HYPERBOLE',\n",
    "    'OTHER',\n",
    "    'ANALOGY',\n",
    "    'EUPHEMISM',\n",
    "    'IM:FALSE ASSERTION',\n",
    "    'EX:CONTEXT SHIFT',\n",
    "    'EX:OXYMORON PARADOX'\n",
    "]\n",
    "\n",
    "for model in models_generations:\n",
    "    #for gen in models_generations[model]:\n",
    "    print(model)\n",
    "    df = models_generations[model]['decoding-1']\n",
    "        \n",
    "    y_true = df['actual']\n",
    "    y_pred = df['prediction']\n",
    "\n",
    "    report = classification_report(y_true, y_pred, labels=labels, digits=4, zero_division=0.0) #output_dict=True\n",
    "    print(report)\n",
    "    #break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
