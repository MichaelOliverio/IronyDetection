{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per aprire i file in una directory e ordinarli\n",
    "def open_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_sig(x, sig=3):\n",
    "    if isinstance(x, (float, int)):\n",
    "        return float(f\"{x:.{sig}g}\")\n",
    "    return x\n",
    "\n",
    "def format_report(report_dict):\n",
    "    rounded = {}\n",
    "    for label, metrics in report_dict.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            rounded[label] = {k: round_sig(v) for k, v in metrics.items()}\n",
    "        else:\n",
    "            rounded[label] = round_sig(metrics)\n",
    "    return rounded\n",
    "\n",
    "def print_formatted_report(report_dict):\n",
    "    print(\"\\nðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\")\n",
    "    labels = [label for label in report_dict if label not in ('accuracy', 'macro avg', 'weighted avg')]\n",
    "    header = f\"{'Label':<20} {'Prec':>8} {'Rec':>8} {'F1':>8} {'Support':>8}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for label in labels + ['macro avg', 'weighted avg']:\n",
    "        row = report_dict[label]\n",
    "        print(f\"{label:<20} {row['precision']:>8.3f} {row['recall']:>8.3f} {row['f1-score']:>8.3f} {row['support']:>8.0f}\")\n",
    "    print(f\"{'Accuracy':<20} {'':>8} {'':>8} {'':>8} {report_dict['accuracy']:>8.3f}\")\n",
    "\n",
    "def calculate_metrics(df, print_confusion=False):\n",
    "    # Assumiamo che prediction e actual siano le colonne corrette\n",
    "    df['extracted_prediction'] = df['prediction'].astype(str)\n",
    "    df['rhetorical_figure'] = df['actual'].astype(str)  # assicurati che 'actual' sia la colonna giusta\n",
    "    \n",
    "    # Rimuoviamo eventuali righe con valori mancanti nelle due colonne (evita errori)\n",
    "    df_clean = df\n",
    "\n",
    "    if print_confusion:\n",
    "        print(f\"\\nðŸ§© Matrice di confusione:\")\n",
    "        confusion = pd.crosstab(df_clean['rhetorical_figure'], df_clean['extracted_prediction'],\n",
    "                                rownames=['Actual'], colnames=['Predicted'])\n",
    "        print(confusion)\n",
    "\n",
    "    report = classification_report(df_clean['rhetorical_figure'], df_clean['extracted_prediction'], output_dict=True, zero_division=0)\n",
    "    return report\n",
    "\n",
    "def average_reports(reports):\n",
    "    avg_report = {}\n",
    "    keys = reports[0].keys()\n",
    "\n",
    "    for key in keys:\n",
    "        if isinstance(reports[0][key], dict):\n",
    "            avg_report[key] = {}\n",
    "            for metric in reports[0][key]:\n",
    "                values = [r[key].get(metric, 0.0) for r in reports if key in r]\n",
    "                avg_report[key][metric] = np.mean(values)\n",
    "        else:  # accuracy\n",
    "            values = [r.get(key, 0.0) for r in reports]\n",
    "            avg_report[key] = np.mean(values)\n",
    "\n",
    "    return avg_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_predictions.csv\n",
      "\n",
      "ðŸ“Š Report per il modello 'LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_predictions.csv' con file 'female_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.276    0.471    0.348       17\n",
      "CONTEXT SHIFT           0.391    0.214    0.277       42\n",
      "EUPHEMISM               0.500    0.167    0.250        6\n",
      "FALSE ASSERTION         0.200    0.077    0.111       13\n",
      "HYPERBOLE               0.500    0.167    0.250        6\n",
      "OTHER                   0.318    0.350    0.333       20\n",
      "OXYMORON                0.037    0.333    0.067        3\n",
      "RHETORICAL QUESTION     0.500    0.400    0.444       15\n",
      "macro avg               0.340    0.272    0.260      122\n",
      "weighted avg            0.358    0.279    0.291      122\n",
      "Accuracy                                           0.279\n",
      "\n",
      "ðŸ“Š Report per il modello 'LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_predictions.csv' con file 'genx_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.276    0.348    0.308       23\n",
      "CONTEXT SHIFT           0.304    0.163    0.212       43\n",
      "EUPHEMISM               0.500    0.053    0.095       19\n",
      "FALSE ASSERTION         0.000    0.000    0.000        6\n",
      "HYPERBOLE               0.500    0.200    0.286        5\n",
      "OTHER                   0.091    0.154    0.114       13\n",
      "OXYMORON                0.037    1.000    0.071        1\n",
      "RHETORICAL QUESTION     0.333    0.333    0.333       12\n",
      "macro avg               0.255    0.281    0.177      122\n",
      "weighted avg            0.300    0.197    0.205      122\n",
      "Accuracy                                           0.197\n",
      "\n",
      "ðŸ“Š Report per il modello 'LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_predictions.csv' con file 'geny_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.241    0.500    0.326       14\n",
      "CONTEXT SHIFT           0.261    0.250    0.255       24\n",
      "EUPHEMISM               0.500    0.250    0.333        4\n",
      "FALSE ASSERTION         0.400    0.091    0.148       22\n",
      "HYPERBOLE               0.500    0.091    0.154       11\n",
      "OTHER                   0.273    0.200    0.231       30\n",
      "OXYMORON                0.037    0.167    0.061        6\n",
      "RHETORICAL QUESTION     0.250    0.273    0.261       11\n",
      "macro avg               0.308    0.228    0.221      122\n",
      "weighted avg            0.304    0.221    0.222      122\n",
      "Accuracy                                           0.221\n",
      "\n",
      "ðŸ“Š Report per il modello 'LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_predictions.csv' con file 'genz_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.345    0.500    0.408       20\n",
      "CONTEXT SHIFT           0.478    0.333    0.393       33\n",
      "EUPHEMISM               0.000    0.000    0.000        8\n",
      "FALSE ASSERTION         0.200    0.036    0.061       28\n",
      "HYPERBOLE               0.500    0.083    0.143       12\n",
      "OTHER                   0.227    0.417    0.294       12\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.333    0.500    0.400        8\n",
      "macro avg               0.260    0.234    0.212      122\n",
      "weighted avg            0.325    0.262    0.256      122\n",
      "Accuracy                                           0.262\n",
      "\n",
      "ðŸ“Š Report per il modello 'LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_predictions.csv' con file 'global_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.172    0.455    0.250       11\n",
      "CONTEXT SHIFT           0.217    0.179    0.196       28\n",
      "EUPHEMISM               0.500    0.143    0.222        7\n",
      "FALSE ASSERTION         0.400    0.105    0.167       19\n",
      "HYPERBOLE               0.500    0.091    0.154       11\n",
      "OTHER                   0.318    0.280    0.298       25\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.417    0.250    0.312       20\n",
      "macro avg               0.316    0.188    0.200      122\n",
      "weighted avg            0.335    0.213    0.232      122\n",
      "Accuracy                                           0.213\n",
      "\n",
      "ðŸ“Š Report per il modello 'LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_predictions.csv' con file 'male_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.172    0.278    0.213       18\n",
      "CONTEXT SHIFT           0.130    0.200    0.158       15\n",
      "EUPHEMISM               0.500    0.100    0.167       10\n",
      "FALSE ASSERTION         0.200    0.048    0.077       21\n",
      "HYPERBOLE               0.500    0.071    0.125       14\n",
      "OTHER                   0.318    0.259    0.286       27\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.417    0.312    0.357       16\n",
      "macro avg               0.280    0.159    0.173      122\n",
      "weighted avg            0.299    0.189    0.202      122\n",
      "Accuracy                                           0.189\n",
      "Processing model: Llama-3.1-8B-Instruct-decoding-1 .csv_predictions.csv\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct-decoding-1 .csv_predictions.csv' con file 'female_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "A PARTICULAR ASPECT OF LANGUAGE CALLED HYPERBOLE    0.000    0.000    0.000        0\n",
      "ANALOGY                 0.241    0.412    0.304       17\n",
      "CONTEXT SHIFT           0.316    0.143    0.197       42\n",
      "EUPHEMISM               0.200    0.333    0.250        6\n",
      "FALSE ASSERTION         0.067    0.077    0.071       13\n",
      "HYPERBOLE               1.000    0.167    0.286        6\n",
      "INTERROGATIVE           0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       20\n",
      "OXYMORON                0.000    0.000    0.000        3\n",
      "RHETORICAL QUESTION     0.294    0.667    0.408       15\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.193    0.163    0.138      122\n",
      "weighted avg            0.245    0.221    0.194      122\n",
      "Accuracy                                           0.221\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct-decoding-1 .csv_predictions.csv' con file 'genx_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "A PARTICULAR ASPECT OF LANGUAGE CALLED HYPERBOLE    0.000    0.000    0.000        0\n",
      "ANALOGY                 0.345    0.435    0.385       23\n",
      "CONTEXT SHIFT           0.368    0.163    0.226       43\n",
      "EUPHEMISM               0.300    0.158    0.207       19\n",
      "FALSE ASSERTION         0.133    0.333    0.190        6\n",
      "HYPERBOLE               1.000    0.200    0.333        5\n",
      "INTERROGATIVE           0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       13\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.265    0.750    0.391       12\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.219    0.185    0.157      122\n",
      "weighted avg            0.315    0.262    0.246      122\n",
      "Accuracy                                           0.262\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct-decoding-1 .csv_predictions.csv' con file 'geny_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "A PARTICULAR ASPECT OF LANGUAGE CALLED HYPERBOLE    0.000    0.000    0.000        0\n",
      "ANALOGY                 0.276    0.571    0.372       14\n",
      "CONTEXT SHIFT           0.211    0.167    0.186       24\n",
      "EUPHEMISM               0.200    0.500    0.286        4\n",
      "FALSE ASSERTION         0.333    0.227    0.270       22\n",
      "HYPERBOLE               1.000    0.091    0.167       11\n",
      "INTERROGATIVE           0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       30\n",
      "OXYMORON                0.000    0.000    0.000        6\n",
      "RHETORICAL QUESTION     0.176    0.545    0.267       11\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.200    0.191    0.141      122\n",
      "weighted avg            0.246    0.213    0.176      122\n",
      "Accuracy                                           0.213\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct-decoding-1 .csv_predictions.csv' con file 'genz_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "A PARTICULAR ASPECT OF LANGUAGE CALLED HYPERBOLE    0.000    0.000    0.000        0\n",
      "ANALOGY                 0.310    0.450    0.367       20\n",
      "CONTEXT SHIFT           0.263    0.152    0.192       33\n",
      "EUPHEMISM               0.100    0.125    0.111        8\n",
      "FALSE ASSERTION         0.267    0.143    0.186       28\n",
      "HYPERBOLE               1.000    0.083    0.154       12\n",
      "INTERROGATIVE           0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       12\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.176    0.750    0.286        8\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.192    0.155    0.118      122\n",
      "weighted avg            0.300    0.213    0.196      122\n",
      "Accuracy                                           0.213\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct-decoding-1 .csv_predictions.csv' con file 'global_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "A PARTICULAR ASPECT OF LANGUAGE CALLED HYPERBOLE    0.000    0.000    0.000        0\n",
      "ANALOGY                 0.276    0.727    0.400       11\n",
      "CONTEXT SHIFT           0.316    0.214    0.255       28\n",
      "EUPHEMISM               0.200    0.286    0.235        7\n",
      "FALSE ASSERTION         0.200    0.158    0.176       19\n",
      "HYPERBOLE               1.000    0.091    0.167       11\n",
      "INTERROGATIVE           0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       25\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.382    0.650    0.481       20\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.216    0.193    0.156      122\n",
      "weighted avg            0.293    0.270    0.230      122\n",
      "Accuracy                                           0.270\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct-decoding-1 .csv_predictions.csv' con file 'male_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "A PARTICULAR ASPECT OF LANGUAGE CALLED HYPERBOLE    0.000    0.000    0.000        0\n",
      "ANALOGY                 0.310    0.500    0.383       18\n",
      "CONTEXT SHIFT           0.053    0.067    0.059       15\n",
      "EUPHEMISM               0.300    0.300    0.300       10\n",
      "FALSE ASSERTION         0.333    0.238    0.278       21\n",
      "HYPERBOLE               1.000    0.071    0.133       14\n",
      "INTERROGATIVE           0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       27\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.265    0.562    0.360       16\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.206    0.158    0.138      122\n",
      "weighted avg            0.284    0.230    0.199      122\n",
      "Accuracy                                           0.230\n",
      "Processing model: Llama-3.1-8B-Instruct_predictions.csv\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct_predictions.csv' con file 'female_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.208    0.294    0.244       17\n",
      "CONTEXT SHIFT           0.368    0.167    0.230       42\n",
      "EUPHEMISM               0.300    0.500    0.375        6\n",
      "FALSE ASSERTION         0.000    0.000    0.000       13\n",
      "HYPERBOLE               0.500    0.167    0.250        6\n",
      "OTHER                   0.000    0.000    0.000       20\n",
      "OXYMORON                0.000    0.000    0.000        3\n",
      "RHETORICAL QUESTION     0.237    0.600    0.340       15\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.179    0.192    0.160      122\n",
      "weighted avg            0.224    0.205    0.185      122\n",
      "Accuracy                                           0.205\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct_predictions.csv' con file 'genx_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.292    0.304    0.298       23\n",
      "CONTEXT SHIFT           0.474    0.209    0.290       43\n",
      "EUPHEMISM               0.400    0.211    0.276       19\n",
      "FALSE ASSERTION         0.000    0.000    0.000        6\n",
      "HYPERBOLE               0.500    0.200    0.286        5\n",
      "OTHER                   0.000    0.000    0.000       13\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.184    0.583    0.280       12\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.206    0.168    0.159      122\n",
      "weighted avg            0.323    0.230    0.241      122\n",
      "Accuracy                                           0.230\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct_predictions.csv' con file 'geny_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.292    0.500    0.368       14\n",
      "CONTEXT SHIFT           0.158    0.125    0.140       24\n",
      "EUPHEMISM               0.300    0.750    0.429        4\n",
      "FALSE ASSERTION         0.500    0.136    0.214       22\n",
      "HYPERBOLE               0.500    0.091    0.154       11\n",
      "OTHER                   0.000    0.000    0.000       30\n",
      "OXYMORON                0.071    0.167    0.100        6\n",
      "RHETORICAL QUESTION     0.211    0.727    0.327       11\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.226    0.277    0.192      122\n",
      "weighted avg            0.232    0.213    0.171      122\n",
      "Accuracy                                           0.213\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct_predictions.csv' con file 'genz_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.250    0.300    0.273       20\n",
      "CONTEXT SHIFT           0.211    0.121    0.154       33\n",
      "EUPHEMISM               0.100    0.125    0.111        8\n",
      "FALSE ASSERTION         0.000    0.000    0.000       28\n",
      "HYPERBOLE               0.500    0.083    0.143       12\n",
      "OTHER                   0.000    0.000    0.000       12\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.158    0.750    0.261        8\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.135    0.153    0.105      122\n",
      "weighted avg            0.164    0.148    0.125      122\n",
      "Accuracy                                           0.148\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct_predictions.csv' con file 'global_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.250    0.545    0.343       11\n",
      "CONTEXT SHIFT           0.158    0.107    0.128       28\n",
      "EUPHEMISM               0.300    0.429    0.353        7\n",
      "FALSE ASSERTION         0.500    0.158    0.240       19\n",
      "HYPERBOLE               0.500    0.091    0.154       11\n",
      "OTHER                   0.000    0.000    0.000       25\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.316    0.600    0.414       20\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.225    0.214    0.181      122\n",
      "weighted avg            0.251    0.230    0.200      122\n",
      "Accuracy                                           0.230\n",
      "\n",
      "ðŸ“Š Report per il modello 'Llama-3.1-8B-Instruct_predictions.csv' con file 'male_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.333    0.444    0.381       18\n",
      "CONTEXT SHIFT           0.053    0.067    0.059       15\n",
      "EUPHEMISM               0.300    0.300    0.300       10\n",
      "FALSE ASSERTION         0.333    0.095    0.148       21\n",
      "HYPERBOLE               0.500    0.071    0.125       14\n",
      "OTHER                   0.000    0.000    0.000       27\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.237    0.562    0.333       16\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.195    0.171    0.150      122\n",
      "weighted avg            0.226    0.197    0.172      122\n",
      "Accuracy                                           0.197\n",
      "Processing model: Minerva-7B-instruct-v1.0_predictions.csv\n",
      "\n",
      "ðŸ“Š Report per il modello 'Minerva-7B-instruct-v1.0_predictions.csv' con file 'female_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.154    0.118    0.133       17\n",
      "CONTEXT SHIFT           0.419    0.429    0.424       42\n",
      "EUPHEMISM               0.000    0.000    0.000        6\n",
      "FALSE ASSERTION         0.200    0.154    0.174       13\n",
      "HYPERBOLE               0.000    0.000    0.000        6\n",
      "OTHER                   0.500    0.050    0.091       20\n",
      "OXYMORON                0.029    0.333    0.054        3\n",
      "RHETORICAL QUESTION     0.143    0.133    0.138       15\n",
      "macro avg               0.181    0.152    0.127      122\n",
      "weighted avg            0.287    0.213    0.216      122\n",
      "Accuracy                                           0.213\n",
      "\n",
      "ðŸ“Š Report per il modello 'Minerva-7B-instruct-v1.0_predictions.csv' con file 'genx_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.462    0.261    0.333       23\n",
      "CONTEXT SHIFT           0.395    0.395    0.395       43\n",
      "EUPHEMISM               0.500    0.053    0.095       19\n",
      "FALSE ASSERTION         0.000    0.000    0.000        6\n",
      "HYPERBOLE               0.000    0.000    0.000        5\n",
      "OTHER                   0.000    0.000    0.000       13\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.214    0.250    0.231       12\n",
      "macro avg               0.196    0.120    0.132      122\n",
      "weighted avg            0.325    0.221    0.240      122\n",
      "Accuracy                                           0.221\n",
      "\n",
      "ðŸ“Š Report per il modello 'Minerva-7B-instruct-v1.0_predictions.csv' con file 'geny_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.308    0.286    0.296       14\n",
      "CONTEXT SHIFT           0.233    0.417    0.299       24\n",
      "EUPHEMISM               0.000    0.000    0.000        4\n",
      "FALSE ASSERTION         0.300    0.136    0.188       22\n",
      "HYPERBOLE               0.000    0.000    0.000       11\n",
      "OTHER                   0.500    0.033    0.062       30\n",
      "OXYMORON                0.059    0.333    0.100        6\n",
      "RHETORICAL QUESTION     0.214    0.273    0.240       11\n",
      "macro avg               0.202    0.185    0.148      122\n",
      "weighted avg            0.280    0.189    0.168      122\n",
      "Accuracy                                           0.189\n",
      "\n",
      "ðŸ“Š Report per il modello 'Minerva-7B-instruct-v1.0_predictions.csv' con file 'genz_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.308    0.200    0.242       20\n",
      "CONTEXT SHIFT           0.302    0.394    0.342       33\n",
      "EUPHEMISM               0.000    0.000    0.000        8\n",
      "FALSE ASSERTION         0.100    0.036    0.053       28\n",
      "HYPERBOLE               0.250    0.083    0.125       12\n",
      "OTHER                   0.000    0.000    0.000       12\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.143    0.250    0.182        8\n",
      "macro avg               0.138    0.120    0.118      122\n",
      "weighted avg            0.189    0.172    0.169      122\n",
      "Accuracy                                           0.172\n",
      "\n",
      "ðŸ“Š Report per il modello 'Minerva-7B-instruct-v1.0_predictions.csv' con file 'global_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.385    0.455    0.417       11\n",
      "CONTEXT SHIFT           0.256    0.393    0.310       28\n",
      "EUPHEMISM               0.000    0.000    0.000        7\n",
      "FALSE ASSERTION         0.100    0.053    0.069       19\n",
      "HYPERBOLE               0.250    0.091    0.133       11\n",
      "OTHER                   0.500    0.040    0.074       25\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.286    0.200    0.235       20\n",
      "macro avg               0.222    0.154    0.155      122\n",
      "weighted avg            0.281    0.189    0.185      122\n",
      "Accuracy                                           0.189\n",
      "\n",
      "ðŸ“Š Report per il modello 'Minerva-7B-instruct-v1.0_predictions.csv' con file 'male_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.615    0.444    0.516       18\n",
      "CONTEXT SHIFT           0.116    0.333    0.172       15\n",
      "EUPHEMISM               0.000    0.000    0.000       10\n",
      "FALSE ASSERTION         0.100    0.048    0.065       21\n",
      "HYPERBOLE               0.500    0.143    0.222       14\n",
      "OTHER                   0.500    0.037    0.069       27\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.286    0.250    0.267       16\n",
      "macro avg               0.265    0.157    0.164      122\n",
      "weighted avg            0.328    0.172    0.184      122\n",
      "Accuracy                                           0.172\n",
      "Processing model: Ministral-8B-Instruct-2410-decoding-1 .csv_predictions.csv\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410-decoding-1 .csv_predictions.csv' con file 'female_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.290    0.529    0.375       17\n",
      "CONTEXT SHIFT           0.286    0.048    0.082       42\n",
      "EUPHEMISM               0.231    0.500    0.316        6\n",
      "FALSE ASSERTION         0.333    0.231    0.273       13\n",
      "HYPERBOLE               0.125    0.167    0.143        6\n",
      "IM:OFFENSIVE LANGUAGE    0.000    0.000    0.000        0\n",
      "IM:RHETORICAL DEVICE    0.000    0.000    0.000        0\n",
      "OTHER                   0.600    0.300    0.400       20\n",
      "OXYMORON                0.000    0.000    0.000        3\n",
      "RHETORICAL QUESTION     0.333    0.733    0.458       15\n",
      "SATIRIC HUMOR           0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.183    0.209    0.171      122\n",
      "weighted avg            0.331    0.287    0.254      122\n",
      "Accuracy                                           0.287\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410-decoding-1 .csv_predictions.csv' con file 'genx_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.419    0.565    0.481       23\n",
      "CONTEXT SHIFT           0.286    0.046    0.080       43\n",
      "EUPHEMISM               0.615    0.421    0.500       19\n",
      "FALSE ASSERTION         0.000    0.000    0.000        6\n",
      "HYPERBOLE               0.125    0.200    0.154        5\n",
      "IM:OFFENSIVE LANGUAGE    0.000    0.000    0.000        0\n",
      "IM:RHETORICAL DEVICE    0.000    0.000    0.000        0\n",
      "OTHER                   0.200    0.154    0.174       13\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.273    0.750    0.400       12\n",
      "SATIRIC HUMOR           0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.160    0.178    0.149      122\n",
      "weighted avg            0.329    0.287    0.261      122\n",
      "Accuracy                                           0.287\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410-decoding-1 .csv_predictions.csv' con file 'geny_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.258    0.571    0.356       14\n",
      "CONTEXT SHIFT           0.429    0.125    0.194       24\n",
      "EUPHEMISM               0.231    0.750    0.353        4\n",
      "FALSE ASSERTION         0.556    0.227    0.323       22\n",
      "HYPERBOLE               0.500    0.364    0.421       11\n",
      "IM:OFFENSIVE LANGUAGE    0.000    0.000    0.000        0\n",
      "IM:RHETORICAL DEVICE    0.000    0.000    0.000        0\n",
      "OTHER                   0.300    0.100    0.150       30\n",
      "OXYMORON                0.000    0.000    0.000        6\n",
      "RHETORICAL QUESTION     0.303    0.909    0.455       11\n",
      "SATIRIC HUMOR           0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.215    0.254    0.188      122\n",
      "weighted avg            0.368    0.295    0.264      122\n",
      "Accuracy                                           0.295\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410-decoding-1 .csv_predictions.csv' con file 'genz_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.355    0.550    0.431       20\n",
      "CONTEXT SHIFT           0.143    0.030    0.050       33\n",
      "EUPHEMISM               0.231    0.375    0.286        8\n",
      "FALSE ASSERTION         0.222    0.071    0.108       28\n",
      "HYPERBOLE               0.250    0.167    0.200       12\n",
      "IM:OFFENSIVE LANGUAGE    0.000    0.000    0.000        0\n",
      "IM:RHETORICAL DEVICE    0.000    0.000    0.000        0\n",
      "OTHER                   0.400    0.333    0.364       12\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.242    1.000    0.390        8\n",
      "SATIRIC HUMOR           0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.154    0.211    0.152      122\n",
      "weighted avg            0.243    0.254    0.209      122\n",
      "Accuracy                                           0.254\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410-decoding-1 .csv_predictions.csv' con file 'global_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.258    0.727    0.381       11\n",
      "CONTEXT SHIFT           0.286    0.071    0.114       28\n",
      "EUPHEMISM               0.385    0.714    0.500        7\n",
      "FALSE ASSERTION         0.444    0.211    0.286       19\n",
      "HYPERBOLE               0.250    0.182    0.211       11\n",
      "IM:OFFENSIVE LANGUAGE    0.000    0.000    0.000        0\n",
      "IM:RHETORICAL DEVICE    0.000    0.000    0.000        0\n",
      "OTHER                   0.500    0.200    0.286       25\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.424    0.700    0.528       20\n",
      "SATIRIC HUMOR           0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.212    0.234    0.192      122\n",
      "weighted avg            0.375    0.328    0.298      122\n",
      "Accuracy                                           0.328\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410-decoding-1 .csv_predictions.csv' con file 'male_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.323    0.556    0.408       18\n",
      "CONTEXT SHIFT           0.000    0.000    0.000       15\n",
      "EUPHEMISM               0.308    0.400    0.348       10\n",
      "FALSE ASSERTION         0.333    0.143    0.200       21\n",
      "HYPERBOLE               0.250    0.143    0.182       14\n",
      "IM:OFFENSIVE LANGUAGE    0.000    0.000    0.000        0\n",
      "IM:RHETORICAL DEVICE    0.000    0.000    0.000        0\n",
      "OTHER                   0.500    0.185    0.270       27\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.394    0.812    0.531       16\n",
      "SATIRIC HUMOR           0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.176    0.187    0.162      122\n",
      "weighted avg            0.321    0.303    0.273      122\n",
      "Accuracy                                           0.303\n",
      "Processing model: Ministral-8B-Instruct-2410_predictions.csv\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410_predictions.csv' con file 'female_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.308    0.235    0.267       17\n",
      "CONTEXT SHIFT           0.278    0.238    0.256       42\n",
      "EUPHEMISM               0.333    0.167    0.222        6\n",
      "FALSE ASSERTION         0.154    0.154    0.154       13\n",
      "HYPERBOLE               0.000    0.000    0.000        6\n",
      "OTHER                   0.600    0.300    0.400       20\n",
      "OXYMORON                0.033    0.333    0.061        3\n",
      "RHETORICAL QUESTION     0.312    0.333    0.323       15\n",
      "macro avg               0.252    0.220    0.210      122\n",
      "weighted avg            0.309    0.238    0.259      122\n",
      "Accuracy                                           0.238\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410_predictions.csv' con file 'genx_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.462    0.261    0.333       23\n",
      "CONTEXT SHIFT           0.278    0.233    0.253       43\n",
      "EUPHEMISM               0.333    0.053    0.091       19\n",
      "FALSE ASSERTION         0.077    0.167    0.105        6\n",
      "HYPERBOLE               0.000    0.000    0.000        5\n",
      "OTHER                   0.200    0.154    0.174       13\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.188    0.250    0.214       12\n",
      "macro avg               0.192    0.140    0.146      122\n",
      "weighted avg            0.280    0.189    0.211      122\n",
      "Accuracy                                           0.189\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410_predictions.csv' con file 'geny_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.385    0.357    0.370       14\n",
      "CONTEXT SHIFT           0.250    0.375    0.300       24\n",
      "EUPHEMISM               0.333    0.250    0.286        4\n",
      "FALSE ASSERTION         0.308    0.182    0.229       22\n",
      "HYPERBOLE               0.000    0.000    0.000       11\n",
      "OTHER                   0.500    0.167    0.250       30\n",
      "OXYMORON                0.067    0.333    0.111        6\n",
      "RHETORICAL QUESTION     0.188    0.273    0.222       11\n",
      "macro avg               0.254    0.242    0.221      122\n",
      "weighted avg            0.303    0.238    0.239      122\n",
      "Accuracy                                           0.238\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410_predictions.csv' con file 'genz_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.462    0.300    0.364       20\n",
      "CONTEXT SHIFT           0.222    0.242    0.232       33\n",
      "EUPHEMISM               0.000    0.000    0.000        8\n",
      "FALSE ASSERTION         0.231    0.107    0.146       28\n",
      "HYPERBOLE               0.000    0.000    0.000       12\n",
      "OTHER                   0.300    0.250    0.273       12\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.125    0.250    0.167        8\n",
      "macro avg               0.167    0.144    0.148      122\n",
      "weighted avg            0.226    0.180    0.194      122\n",
      "Accuracy                                           0.180\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410_predictions.csv' con file 'global_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.462    0.545    0.500       11\n",
      "CONTEXT SHIFT           0.222    0.286    0.250       28\n",
      "EUPHEMISM               0.333    0.143    0.200        7\n",
      "FALSE ASSERTION         0.308    0.211    0.250       19\n",
      "HYPERBOLE               0.000    0.000    0.000       11\n",
      "OTHER                   0.600    0.240    0.343       25\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "RHETORICAL QUESTION     0.312    0.250    0.278       20\n",
      "macro avg               0.280    0.209    0.228      122\n",
      "weighted avg            0.334    0.246    0.269      122\n",
      "Accuracy                                           0.246\n",
      "\n",
      "ðŸ“Š Report per il modello 'Ministral-8B-Instruct-2410_predictions.csv' con file 'male_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.615    0.444    0.516       18\n",
      "CONTEXT SHIFT           0.139    0.333    0.196       15\n",
      "EUPHEMISM               0.333    0.100    0.154       10\n",
      "FALSE ASSERTION         0.154    0.095    0.118       21\n",
      "HYPERBOLE               0.000    0.000    0.000       14\n",
      "OTHER                   0.600    0.222    0.324       27\n",
      "OXYMORON                0.033    1.000    0.065        1\n",
      "RHETORICAL QUESTION     0.188    0.188    0.188       16\n",
      "macro avg               0.258    0.298    0.195      122\n",
      "weighted avg            0.319    0.213    0.230      122\n",
      "Accuracy                                           0.213\n",
      "Processing model: Qwen2.5-7B-Instruct_predictions.csv\n",
      "\n",
      "ðŸ“Š Report per il modello 'Qwen2.5-7B-Instruct_predictions.csv' con file 'female_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.000    0.000    0.000       17\n",
      "APHORISM                0.000    0.000    0.000        0\n",
      "CONTEXT SHIFT           0.333    0.024    0.044       42\n",
      "EUPHEMISM               0.000    0.000    0.000        6\n",
      "EX:CONTRAST             0.000    0.000    0.000        0\n",
      "EX:OXOP FIGURE          0.000    0.000    0.000        0\n",
      "EXCATHIS FIGURATIVE LANGUAGE IS USED TO MAKE A POINT EXPLICITLY. THAT'S WHY IS AN EXAMPLE OF EOC (EUPHEMISM OR CONCEALMENT)    0.000    0.000    0.000        0\n",
      "EXPLAIN: THE FIGURES ARE CLEARLY OPPOSITE AND ARE ABSTRACT THINGS    0.000    0.000    0.000        0\n",
      "EXPLAIN:OXYMORON FIGURES    0.000    0.000    0.000        0\n",
      "FALSE ASSERTION         0.000    0.000    0.000       13\n",
      "HYPERBOLE               0.000    0.000    0.000        6\n",
      "MINOR PREMISE ABSENT    0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       20\n",
      "OXYMORON                0.050    0.333    0.087        3\n",
      "PARADOX                 0.000    0.000    0.000        0\n",
      "RHETORICAL QUESTION     0.000    0.000    0.000       15\n",
      "VERBAL IRRITY. THAT'S WHY IS AN EXAMPLE OF VERBAL IRRITY    0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.021    0.020    0.007      122\n",
      "weighted avg            0.116    0.016    0.017      122\n",
      "Accuracy                                           0.016\n",
      "\n",
      "ðŸ“Š Report per il modello 'Qwen2.5-7B-Instruct_predictions.csv' con file 'genx_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.000    0.000    0.000       23\n",
      "APHORISM                0.000    0.000    0.000        0\n",
      "CONTEXT SHIFT           0.333    0.023    0.043       43\n",
      "EUPHEMISM               0.000    0.000    0.000       19\n",
      "EX:CONTRAST             0.000    0.000    0.000        0\n",
      "EX:OXOP FIGURE          0.000    0.000    0.000        0\n",
      "EXCATHIS FIGURATIVE LANGUAGE IS USED TO MAKE A POINT EXPLICITLY. THAT'S WHY IS AN EXAMPLE OF EOC (EUPHEMISM OR CONCEALMENT)    0.000    0.000    0.000        0\n",
      "EXPLAIN: THE FIGURES ARE CLEARLY OPPOSITE AND ARE ABSTRACT THINGS    0.000    0.000    0.000        0\n",
      "EXPLAIN:OXYMORON FIGURES    0.000    0.000    0.000        0\n",
      "FALSE ASSERTION         0.000    0.000    0.000        6\n",
      "HYPERBOLE               0.000    0.000    0.000        5\n",
      "MINOR PREMISE ABSENT    0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       13\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "PARADOX                 0.000    0.000    0.000        0\n",
      "RHETORICAL QUESTION     0.200    0.083    0.118       12\n",
      "VERBAL IRRITY. THAT'S WHY IS AN EXAMPLE OF VERBAL IRRITY    0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.030    0.006    0.009      122\n",
      "weighted avg            0.137    0.016    0.027      122\n",
      "Accuracy                                           0.016\n",
      "\n",
      "ðŸ“Š Report per il modello 'Qwen2.5-7B-Instruct_predictions.csv' con file 'geny_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.000    0.000    0.000       14\n",
      "APHORISM                0.000    0.000    0.000        0\n",
      "CONTEXT SHIFT           0.000    0.000    0.000       24\n",
      "EUPHEMISM               0.000    0.000    0.000        4\n",
      "EX:CONTRAST             0.000    0.000    0.000        0\n",
      "EX:OXOP FIGURE          0.000    0.000    0.000        0\n",
      "EXCATHIS FIGURATIVE LANGUAGE IS USED TO MAKE A POINT EXPLICITLY. THAT'S WHY IS AN EXAMPLE OF EOC (EUPHEMISM OR CONCEALMENT)    0.000    0.000    0.000        0\n",
      "EXPLAIN: THE FIGURES ARE CLEARLY OPPOSITE AND ARE ABSTRACT THINGS    0.000    0.000    0.000        0\n",
      "EXPLAIN:OXYMORON FIGURES    0.000    0.000    0.000        0\n",
      "FALSE ASSERTION         0.000    0.000    0.000       22\n",
      "HYPERBOLE               0.000    0.000    0.000       11\n",
      "MINOR PREMISE ABSENT    0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       30\n",
      "OXYMORON                0.050    0.167    0.077        6\n",
      "PARADOX                 0.000    0.000    0.000        0\n",
      "RHETORICAL QUESTION     0.200    0.091    0.125       11\n",
      "VERBAL IRRITY. THAT'S WHY IS AN EXAMPLE OF VERBAL IRRITY    0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.014    0.014    0.011      122\n",
      "weighted avg            0.021    0.016    0.015      122\n",
      "Accuracy                                           0.016\n",
      "\n",
      "ðŸ“Š Report per il modello 'Qwen2.5-7B-Instruct_predictions.csv' con file 'genz_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.000    0.000    0.000       20\n",
      "APHORISM                0.000    0.000    0.000        0\n",
      "CONTEXT SHIFT           0.333    0.030    0.056       33\n",
      "EUPHEMISM               0.000    0.000    0.000        8\n",
      "EX:CONTRAST             0.000    0.000    0.000        0\n",
      "EX:OXOP FIGURE          0.000    0.000    0.000        0\n",
      "EXCATHIS FIGURATIVE LANGUAGE IS USED TO MAKE A POINT EXPLICITLY. THAT'S WHY IS AN EXAMPLE OF EOC (EUPHEMISM OR CONCEALMENT)    0.000    0.000    0.000        0\n",
      "EXPLAIN: THE FIGURES ARE CLEARLY OPPOSITE AND ARE ABSTRACT THINGS    0.000    0.000    0.000        0\n",
      "EXPLAIN:OXYMORON FIGURES    0.000    0.000    0.000        0\n",
      "FALSE ASSERTION         0.000    0.000    0.000       28\n",
      "HYPERBOLE               0.000    0.000    0.000       12\n",
      "MINOR PREMISE ABSENT    0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       12\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "PARADOX                 0.000    0.000    0.000        0\n",
      "RHETORICAL QUESTION     0.000    0.000    0.000        8\n",
      "VERBAL IRRITY. THAT'S WHY IS AN EXAMPLE OF VERBAL IRRITY    0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.018    0.002    0.003      122\n",
      "weighted avg            0.090    0.008    0.015      122\n",
      "Accuracy                                           0.008\n",
      "\n",
      "ðŸ“Š Report per il modello 'Qwen2.5-7B-Instruct_predictions.csv' con file 'global_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.000    0.000    0.000       11\n",
      "APHORISM                0.000    0.000    0.000        0\n",
      "CONTEXT SHIFT           0.000    0.000    0.000       28\n",
      "EUPHEMISM               0.000    0.000    0.000        7\n",
      "EX:CONTRAST             0.000    0.000    0.000        0\n",
      "EX:OXOP FIGURE          0.000    0.000    0.000        0\n",
      "EXCATHIS FIGURATIVE LANGUAGE IS USED TO MAKE A POINT EXPLICITLY. THAT'S WHY IS AN EXAMPLE OF EOC (EUPHEMISM OR CONCEALMENT)    0.000    0.000    0.000        0\n",
      "EXPLAIN: THE FIGURES ARE CLEARLY OPPOSITE AND ARE ABSTRACT THINGS    0.000    0.000    0.000        0\n",
      "EXPLAIN:OXYMORON FIGURES    0.000    0.000    0.000        0\n",
      "FALSE ASSERTION         0.000    0.000    0.000       19\n",
      "HYPERBOLE               0.000    0.000    0.000       11\n",
      "MINOR PREMISE ABSENT    0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       25\n",
      "OXYMORON                0.050    1.000    0.095        1\n",
      "PARADOX                 0.000    0.000    0.000        0\n",
      "RHETORICAL QUESTION     0.000    0.000    0.000       20\n",
      "VERBAL IRRITY. THAT'S WHY IS AN EXAMPLE OF VERBAL IRRITY    0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.003    0.056    0.005      122\n",
      "weighted avg            0.000    0.008    0.001      122\n",
      "Accuracy                                           0.008\n",
      "\n",
      "ðŸ“Š Report per il modello 'Qwen2.5-7B-Instruct_predictions.csv' con file 'male_actual_labels.csv':\n",
      "\n",
      "ðŸ“ˆ Report di classificazione (precision, recall, f1-score, support):\n",
      "Label                    Prec      Rec       F1  Support\n",
      "--------------------------------------------------------\n",
      "ANALOGY                 0.000    0.000    0.000       18\n",
      "APHORISM                0.000    0.000    0.000        0\n",
      "CONTEXT SHIFT           0.000    0.000    0.000       15\n",
      "EUPHEMISM               0.000    0.000    0.000       10\n",
      "EX:CONTRAST             0.000    0.000    0.000        0\n",
      "EX:OXOP FIGURE          0.000    0.000    0.000        0\n",
      "EXCATHIS FIGURATIVE LANGUAGE IS USED TO MAKE A POINT EXPLICITLY. THAT'S WHY IS AN EXAMPLE OF EOC (EUPHEMISM OR CONCEALMENT)    0.000    0.000    0.000        0\n",
      "EXPLAIN: THE FIGURES ARE CLEARLY OPPOSITE AND ARE ABSTRACT THINGS    0.000    0.000    0.000        0\n",
      "EXPLAIN:OXYMORON FIGURES    0.000    0.000    0.000        0\n",
      "FALSE ASSERTION         0.000    0.000    0.000       21\n",
      "HYPERBOLE               0.000    0.000    0.000       14\n",
      "MINOR PREMISE ABSENT    0.000    0.000    0.000        0\n",
      "OTHER                   0.000    0.000    0.000       27\n",
      "OXYMORON                0.000    0.000    0.000        1\n",
      "PARADOX                 0.000    0.000    0.000        0\n",
      "RHETORICAL QUESTION     0.200    0.062    0.095       16\n",
      "VERBAL IRRITY. THAT'S WHY IS AN EXAMPLE OF VERBAL IRRITY    0.000    0.000    0.000        0\n",
      "nan                     0.000    0.000    0.000        0\n",
      "macro avg               0.011    0.003    0.005      122\n",
      "weighted avg            0.026    0.008    0.013      122\n",
      "Accuracy                                           0.008\n"
     ]
    }
   ],
   "source": [
    "model_files = open_files('models_generations')\n",
    "actual_files = open_files('actuals')\n",
    "\n",
    "models = {}\n",
    "for model_file in model_files:\n",
    "    # estrai il nome modello rimuovendo pattern specifici\n",
    "    model_name = re.sub(r\"^(fine-tuned-)?|-decoding-\\d+\\.csv$\", \"\", model_file)\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "\n",
    "    model = pd.read_csv(os.path.join('models_generations', model_file))\n",
    "\n",
    "    reports = []\n",
    "    for actual_file in actual_files:\n",
    "        actual = pd.read_csv(os.path.join('actuals', actual_file))\n",
    "\n",
    "        # fai il join di actual e model, utilizzando la colonna \"pippo\" di actual e l'index di model\n",
    "        merged = pd.merge(actual, model, left_on='index', right_index=True, how='left')\n",
    "\n",
    "        print(f\"\\nðŸ“Š Report per il modello '{model_name}' con file '{actual_file}':\")\n",
    "        report = calculate_metrics(merged, print_confusion=False)\n",
    "        #print(report)\n",
    "        formatted_report = format_report(report)\n",
    "        print_formatted_report(formatted_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
